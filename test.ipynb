{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML  Lab 4 - Multi-class Classification (linear vs nonlinear approach) \n",
    "\n",
    "## PART 1: One-vs-all multi-class classification with Logistic Regression - linear approach\n",
    "Objectives: Implementation of one-vs-all Logistic Regression (linear approach) to recognize hand-written digits (from 0 to 9) from images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks. This assignment will show you how the methods you've learned can be used for this classification task. In this part, you will extend your previous implementation of Logistic Regression and apply it to one-vs-all classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#to load matlab mat files\n",
    "from scipy.io import loadmat\n",
    "import pathlib\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of images = 9008\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import copy\n",
    "from PIL import Image\n",
    "\n",
    "import glob\n",
    "\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path('./dataset')\n",
    "\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.*')))\n",
    "print(\"Total no of images =\", image_count)\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "classes = set()\n",
    "\n",
    "image_width = 30\n",
    "image_height = 30\n",
    "\n",
    "for filename in data_dir.glob('*/*.*'):\n",
    "    class_name = str(filename).split(\"/\")[1]\n",
    "    classes.add(class_name)\n",
    "    im=copy.deepcopy(Image.open(filename).resize((image_width,image_height)).convert('LA'))\n",
    "    #print(np.array(im).shape)\n",
    "    images.append(np.array(im)[...,:1].reshape((image_width*image_height*1,)))\n",
    "    #print(images[0].shape)\n",
    "    labels.append(class_name)\n",
    "    im.close()\n",
    "\n",
    "classes = sorted(list(classes))\n",
    "labels = [i for x in labels for i in range(len(classes)) if x == classes[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00784314 0.05882353 0.18039216 0.35294118 0.6\n",
      " 0.25098039 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.03529412 0.50588235\n",
      " 0.49019608 0.43529412 0.44313725 0.48235294 0.54509804 0.64705882\n",
      " 0.73333333 0.80784314 0.91372549 1.         1.         0.96078431\n",
      " 0.34117647 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.47843137 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.96078431 0.90588235 0.78039216 0.56470588 0.33333333 0.10980392\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.32941176 0.94509804\n",
      " 0.8627451  0.32941176 0.31764706 0.28627451 0.23529412 0.16078431\n",
      " 0.10196078 0.05490196 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.65490196\n",
      " 0.92941176 0.02745098 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.43529412\n",
      " 1.         0.30588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.11764706\n",
      " 0.92941176 0.85098039 0.11372549 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.38431373 1.         0.8627451  0.36862745 0.03921569 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.43529412 0.94509804 1.         0.81176471 0.3254902\n",
      " 0.         0.         0.00392157 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1372549  0.56470588 0.96470588 1.\n",
      " 0.65882353 0.08627451 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.16862745 0.67843137\n",
      " 1.         0.75294118 0.04705882 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.5254902  1.         0.34509804 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.10980392 0.98823529 0.54901961 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.22352941 1.         0.45098039 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09019608\n",
      " 0.76078431 0.98431373 0.18431373 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0745098  0.56078431 0.31764706 0.45490196 0.87843137\n",
      " 1.         0.50196078 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.14901961 0.98823529 1.         1.         0.91372549\n",
      " 0.44705882 0.00784314 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.23921569 0.52941176 0.38431373 0.09411765\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "(9008, 900)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataX = np.array(images).reshape((len(images), images[0].shape[0]))\n",
    "\n",
    "\n",
    "#dataX[dataX <  128] = 0\n",
    "#dataX[dataX >= 128] = 1\n",
    "dataX = 1 - dataX / 255\n",
    "\n",
    "plt.imshow(dataX[0].reshape(image_width, image_height), cmap=\"gray\")\n",
    "print(dataX[0])\n",
    "\n",
    "dataY = np.array(labels).reshape((len(labels),1))\n",
    "print(dataX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9008, 900)\n",
      "(9008, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataX.shape)\n",
    "print(dataY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.60\n",
    "validation_ratio = 0.20\n",
    "test_ratio = 0.20\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "# the _junk suffix means that we drop that variable completely\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataX, dataY, test_size=1 - train_ratio)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
    "\n",
    "#print(x_train, x_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7206, 900)\n",
      "(901, 900)\n",
      "(901, 900)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Cost Function\n",
    "\n",
    "Recall that the regularized cost function in logistic regression is:\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [ -y^{(i)}log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)})log(1 - (h_{\\theta}(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "The cost gradients are (remember that the gradient of $\\theta_0$ is not regularized): \n",
    "\n",
    "$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$ for $j=0$\n",
    "\n",
    "$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j$ for $j\\geq 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sigmoid FUNCTION from the previous lab\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    return the sigmoid of z\n",
    "    \"\"\"\n",
    "    gz= 1/(1+np.exp(-z))\n",
    "    \n",
    "    return gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add costFunctionReg FUNCTION from the previous lab (the function that computes the regularized cost and the gradients)\n",
    "def costFunctionReg(X, y, theta, Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array of  data X, labels y and theta, to return the regularized cost function and gradients\n",
    "    of the logistic regression classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    #number of training examples \n",
    "    m=len(y)\n",
    "        \n",
    "    #vector of the model predictions for all training examples      \n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    \n",
    "    error = (-y * np.log(h)) - ((1-y)*np.log(1-h))\n",
    "    \n",
    "    #cost function without regularization term\n",
    "    cost = sum(error)/m\n",
    "    \n",
    "    #add regularization term to the cost function\n",
    "    regCost= cost + Lambda/(2*m) * sum(theta[1:]**2)\n",
    "    \n",
    "    #gradient of theta_0\n",
    "    grad_0= (1/m) * np.dot(X.transpose(),(h - y))[0]\n",
    "    \n",
    "    #vector of gradients of theta_j from j=1:n (adding the regularization term of the gradient)\n",
    "    grad = (1/m) * np.dot(X.transpose(),(h - y))[1:] + (Lambda/m)* theta[1:]\n",
    "       \n",
    "    # all gradients in a column vector shape\n",
    "    grad_all=np.append(grad_0,grad)\n",
    "    grad_all = grad_all.reshape((len(grad_all), 1))\n",
    "    \n",
    "    return regCost[0], grad_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gradientDescent FUNCTION from the previous lab (the function that returns the optimal theta vector and J_history)\n",
    "def gradientDescent(X,y,theta,alpha,num_iters,Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        #call CostFunctionReg \n",
    "        cost, grad = costFunctionReg(X,y,theta,Lambda)\n",
    "        \n",
    "        #update theta\n",
    "        theta = theta - alpha*grad\n",
    "        \n",
    "        J_history.append(cost)\n",
    "    \n",
    "    return theta , J_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-all Classification\n",
    "\n",
    "You will implement one-vs-all classification by training multiple Regularized Logistic Regression classifiers, one for each of the K classes in the MNIST dataset. \n",
    "\n",
    "In the handwritten digits dataset, K = 10, but your code should work for any value of K. You should complete the code in function *oneVsAll* to train one classier for each class. \n",
    "\n",
    "Your code should return all the classifier parameters in a Kx(n+1) matrix *all_theta*, where each row corresponds to the learned Logistic Regression parameters for one class. You can do this with a for-loop from 1 to K, training each classifier independently.\n",
    "\n",
    "Note that the y argument to this function is a vector of labels from 1 to 10, where we have mapped the digit “0\" to the label 10. When training the classifier for class i you will transform  vector y, with shape (m, 1), in a binary vector, where only for examples with label i, y gets value = 1, and for all other labels (not class i), y gets value = 0. \n",
    "\n",
    "Logical arrays are helpful for this task, e.g. y=np.where(y==i,1,0). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, initial_theta, alpha, num_iters, Lambda, K):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    numpy array of data X and labels y\n",
    "    initial_theta - inicialized vector of model parameters theta \n",
    "    alpha - learning rate\n",
    "    num_iters - number of iterations\n",
    "    Lambda - regularization parameter \n",
    "    K -number of classes\n",
    "    \n",
    "    ONEVSALL trains K Logistic Regression classifiers using gradient descent. \n",
    "    \n",
    "    Returns:   \n",
    "    all_theta - Kxn matrix where i-th row corresponds to the i-th classifier, n parameters\n",
    "    all_J - the evolution of cost function during each iteration (J_history) for all K classifiers\n",
    "    \n",
    "    \"\"\"\n",
    "    all_theta = []\n",
    "    all_J=[]\n",
    "    \n",
    "    #number of training examples\n",
    "    m= len(X)\n",
    "    \n",
    "    #number of features\n",
    "    n= X.shape[1]\n",
    "    \n",
    "    # add an extra column of 1´s corresponding to xo=1 (aka intercept term)\n",
    "    X= np.append(np.ones((X.shape[0],1)),X,axis=1) \n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(0,K):\n",
    "        theta , J_history = gradientDescent(X,np.where(y==i,1,0),initial_theta,alpha,num_iters,Lambda)\n",
    "        \n",
    "        #update (extend)\n",
    "        all_theta.extend(theta) \n",
    "        \n",
    "        #update (extend)\n",
    "        all_J.extend(J_history)\n",
    "        \n",
    "    return np.array(all_theta).reshape(K,n+1), all_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7206\n"
     ]
    }
   ],
   "source": [
    "#Inicialize vector theta =0\n",
    "initial_theta = np.zeros((x_train.shape[1]+1,1))\n",
    "\n",
    "#Optimization hyper-parameters \n",
    "alpha=0.2 #learning rate\n",
    "num_iters=1000\n",
    "Lambda=0.1\n",
    "print(len(y_train))\n",
    "\n",
    "# 4_% __%  30x30  round 1  zeros  0.1    300  0.1\n",
    "# 46% 46%  30x30  round 1  zeros  0.1   1000  0.1\n",
    "# 34% 38%  30x30  round 2  zeros  0.01   400  0.1\n",
    "# 23% __%  40x40  round 1  zeros  0.1   1000  0.1\n",
    "# 32% 23%  40x40  round 2  zeros  0.1   1000  0.1\n",
    "# 39% 40%  40x40  round 2  zeros  0.01   400  0.1\n",
    "all_theta, all_J = oneVsAll(x_train, y_train, initial_theta, alpha, num_iters, Lambda, len(classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Cost functions for all (K =10) classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Cost function using Gradient Descent')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABHkElEQVR4nO2dd3gcxfnHP+9JVpcty5KbZFuyLTfcO2BMsQHTQzctlB8QWgKBhAAhBEhCDQRIILSEODRjMMWAwTiAwTRjGfcud7nKRe5N9vz+2D35JEuydDt7N9LN53n06G53bvbd2dn5zrzTRCmFxWKxWCxBAtE2wGKxWCxmYYXBYrFYLBWwwmCxWCyWClhhsFgsFksFrDBYLBaLpQJWGCwWi8VSASsMlmoRkRtFZL2I7BCRZhG87j0i8nKkrnckROQyEfks2nZ4QUSWi8hw97NR6WsxDysM9QARuVRECt0Ceq2IfCIiQzzGWV5QVHO+EfAkcIpSKk0ptcnL9Wq4zgkiUhx6TCn1kFLqWj+uFw5KqdeVUqf4Fb+IjBSRKSKyU0Q2uJ9vEhHx43q60ldE8kREiUh8DWHuF5H9IrLd/VskIv8QkVZer+8X7j11jLYd0cQKg+GIyO3AU8BDQAugLfAccI7Pl24BJAFzfb5OTCMidwBPA48DLXHS/QbgWCChmt/ERcxAPbyllEoHMoFzce5zmsniEPMopeyfoX9AE2AHcGENYRJxhGON+/cUkOieywI+AkqBzcBknMrAq8BBYLcb/52V4uwE7ASUe/4LIM/9Hh8SbhJwrfv5KuAb4K/AFmAZcFpI2EzgFdfGLcD7QKprw0H3OjuA1sD9wGshvz0bR6BK3Wt2DTm3HPgNMAvYCrwFJFWTVpXjrXBP7j0sBba79l8Wem8hv1M4hfdi16ZnAXHPxQFPABvdOG6pnG6Vnu9O4Pwj5IP/AP8ExrvhhwNnANOBbcAq4P5Kv7kCWAFsAn7vptPwatJhMPCdey8zgRMqPeM/Ad+66fIZkOWeW8mhPLIDOPpIaR6SRjOBv4YcOxOY4drwHdAz5NzvgNXu9RcCw0LiuQdY4p6bBrRxz3UBJuLk+4XARZXS81ngY/d3U4AO7rmv3Xva6d7TxdEuB6JS9kTbAPtXw8OBEUBZVYVKSJgHgR+A5kC2+1L9yT33MPA80Mj9Oy6kACsvKKqJN4+KhWaF7+6xSVQUhv3Ade4LeyOOCASv9zFOod3UteV49/gJQHGla5cXJhwSqZPd390JFAEJIffxI46gZALzgRuquacKhVToPeGI1Dags3uuFXBUyL1VFoaPgAycFlwJMMI9dwMwD8h17/V/ldOtLs/XDfcfHNE7FkfYk9x06+F+7wmsB37mhu+GU6gNxak4POle5zBhAHJwxON0N66T3e/ZIc94ifsckt3vj1SXJ46U5pXy7RT3cx9gAzAIJ+9c6T7XRKAzjvC1DrlmsBD/LTDbDSNAL6CZ+yxXAVe7z7YPjlB3C0nPTcBA9/zrwOhKz7djtN//aP5ZV5LZNAM2KqXKaghzGfCgUmqDUqoEeACntghOQd0KaKeU2q+UmqzcnO8TK5RSLymlDgCj3Gu3cF0Gp+EU2FtcW76qZZwXAx8rpSYqpfbjtEiSgWNCwjyjlFqjlNoMfAj0DtP+g0B3EUlWSq1VStXkRntEKVWqlFoJfBlyzYuAp5VSxUqpLcAjNcSRRaXnKyLfiUipiOwWkaEhYT9QSn2rlDqolNqjlJqklJrtfp8FvAkc74a9APhIKfW1Umov8Af33qricmC8Umq8G9dEoBBHKIK8opRapJTaDYwh/PQNZQ2OkANcD7yglJqilDqglBoF7MVpyRzAEYhuItJIKbVcKbXE/d21wL1KqYXKYaZy+sLOBJYrpV5RSpUppaYDY4ELQ67/nlLqRzftX9d0Tw0GKwxmswnIqqlzD6emvCLk+wr3GDh+6yLgMxFZKiJ3+WNmOeuCH5RSu9yPaUAbYLNbUNaVCvenlDqIUxvMqeq6wC73mnVCKbUTR4RuANaKyMci0qWGn1R3zdaufUFCP1fmsOerlDpGKZXhngt9PyvEIyKDRORLESkRka2u3VlV2eDeW3WDB9oBF7piVCoipcAQHFE/0r16IQfHzRO04Y5KNrTBaSUUAbfhtDw2iMhoEQnm7zY4rZmq7mlQpfguw+nb8POeGgxWGMzme5ya089qCLMG50UI0tY9hlJqu1LqDqVUexw//e0iMswNV9eWw073f0rIsZZVBayCVUCmiGRUce5IdlS4P3ekThscn3Nd2UkN9iulJiilTsYpFBcAL4VxjbU4bqQgbWoIG3y+tRlIUDmd3gDG4fjUm+C4DIOjmNaGXldEUnBan1WxCnhVKZUR8peqlKqppVOdTbVCRALAWTh9XkEb/lLJhhSl1JsASqk3lFJDcPKBAh4N+V2Hau7pq0rxpSmlbgzH3ljECoPBKKW2AvcBz4rIz0QkRUQaichpIvKYG+xN4F4RyRaRLDf8awAicqaIdHQL0604zfKgS2E90L4OtpTgFMaXi0iciFxD1S9lVb9dC3wCPCciTd17CLpJ1gPNRKRJNT8fA5whIsPcIbR34BSm39XW9hBmAENFpK17vbuDJ0SkhYicIyKpbvw7qN79UhNjgFtFJMcVwt9VF1ApVYrj+ntORC4QkXQRCYhIbxw/eU2k47TC9ojIQODSkHPvAGeKyBARScDx51f3rr8GnCUip7rPNckdQpxbTfhQSnDSqFb5SETiRaQrTp5tidP3AY4A3+C2gkREUkXkDDc9OovISSKSCOzh0GAFgJeBP4lIgfu7nuLMt/kI6CQiV7h5rZGIDHCvXRvq9G40RKwwGI5S6gngduBenBdxFc5Il/fdIH/G8QnPwumI+8k9BlCA0/m5A6d2+pxS6kv33MM4glIqIr+ppTnX4XT4bQKOom6F8xU4fR4LcDoab3PvbwFOQbHUtaV16I+UUgtx/OB/x+lAPAs4Sym1rw7XDsY1EacDfBbOCJaPQk4HcNJ5DY6L43icDvS68hLOyJ1ZOKOGxuN0/B6oxqbH3OveiVMgrQdewBGUmtL3JuBBEdmOUxkYExLnXOBmnFbFWpxRYMVVRaKUWoXTYrmHQ/nrt9SibHDdhX8BvnWf3eBqgl4sIjtwKifjcPJPP6VUsGVbiJO3/uHaWoTT4Q9O/8IjOM9+Hc4gi6CgP+ne92c4Awf+BSQrpbYDpwAjcZ7nOpxWRuKR7snlfmCUe08X1fI3DYrgiBGLxeIDInIa8LxSqt0RA1sshmBbDBaLRkQkWUROd90mOcAfgfeibZfFUhdsi8Fi0Yjb0fsVzgSr3TjzN25VSm2LqmEWSx2wwmCxWCyWClhXksVisVgqUNPEqXpBVlaWysvLi7YZFovFUq+YNm3aRqVUdlXn6r0w5OXlUVhYGG0zLBaLpV4hIiuqO2ddSRaLxWKpgBUGi8VisVTACoPFYrFYKhBRYRCRESKyUESKqlrpU0T+JiIz3L9F7qqIFovFYokgEet8drcjfBZnI5BiYKqIjFNKzQuGUUr9OiT8L3E22LBYLBZLBIlki2EgUKSUWuougDaampcbvgRncTWLxWKxRJBICkMOFTcbKabiZivliEg7IB9nr+Gqzl8vIoUiUlhSUqLdUIvFYollTO18Hgm8424ReRhKqReVUv2VUv2zs6ucn3FEpi7fzJOfLWRfWThL7ldky859fDxrred4ALbu2s+HM9doiWv7nv18MCOc/WwOZ9e+Mt6bXuXKzXVmz/4DjJ1WjI7lWPaWHeDtwlVa4tpXdpAxhas4eNB7XGUHDjJmqp64DhxUjJm6irID3vPqwYOKMYWr2K8hLqUUbxeu0vIOKaV4Z1oxe/ZX+crXOa53fypm176adsStPR/MWM2OvXriGjdzDVt379cSl59EUhhWU3E3q1yq34VrJD67kX5asYVnviii7KD3TH3Da9O4+Y2fWLd1j+e4bntrOr98czrLNu48cuAjcNfY2dw6egbz1nhfv+2PH8zl12/NpHD55iMHPgIPjZ/PHW/PZPLijZ7jenLiIn77ziw+m7fec1zPTSrizndmMU6DMP/722XcOXYWbxXWtLNn7XhjygruHDuLUd9XOx+p1rw7fTV3vjOLF79e6jmu8bPX8dt3ZvH054s8x/XVohJ+8/ZMHvlkgee4fly2mdvHzOSBcfOOHPgIzC7eyq2jZ3DPu7M9x1W0YTu/enM6d4yZ6Tkuv4mkMEwFCkQk391VaiTOph0VcPfZbYqzsUy9YHXpbgAttbA1pY647C3zXnNas9Wxa7eGWti6bY5dO/d5j2vDtr1OXBpqYSXbnbi27/Ee16Ydzt4/2/Z4r9Ft2unEVbrLe1xb3DhKd9V5b6LDCMYRvFcvBNNJR1zB51eyY6/nuHa6LYUN271X1IJxBfO/F3bvO+jGtdtzXH4TMWFQSpXh7Dw2AZgPjFFKzRWRB0Xk7JCgI4HRyi77arFYLFEhomslKaXG42x1GHrsvkrf74+sTZG8msVisZiPqZ3PviMSbQsiiVW/aKEMTXtT7TLVLGPt8omYFQbT0dGS8UP7dHr4dL5rWu3Skvb6U1+LXYbWiPwwS0eO8Ocd8iFSzcS8MOgtnLzHYeh7q7VA0XmPOgtgY9M+2gbUM/wQZR2Ymr+qImaFIRYKFIslEtSHGrClbsSsMFgsFm/Y+lDDxQpDDKCzRmdq5dBYu7SmvVn9KH5g6j0a21nvEzEvDKZOlzCto9HUTjhfOi01GKa1H0Vrn4yZaO0X0Jr2pqaYv8SsMMTo8zYCU9PeULMsDQxD66IViFlh8INYa25aLJaGScwLg56xzrauabFYGg4xLwyxgNZ2jKmNIsPsiqk+GWM7jDXGZVj+8hsrDIai42XTWQbYfoHoYWoHaCx0sBua9L5jhcEwTC0E/MDUPhkzrbI0FOpD/op5YYi1JqIJmNonE1OibDN+xKlP2StmhSGWCgFL9DCtADY91xuWXOUYapZvxKww+IGxmToGZoCaZpepq4WaiqkrAZsupH5hhUED/sy+9R6HqR16OrENP0tlrDfAO1YYDKuGxVKWjoUWVixg06vhEbPCEEsFsHHEUOKbWmjaiZ3Rw7R+p6qIWWGIJepDRmxomFpomu5mMa2vKIie/gqz0z6UmBcGUzOiDkzdXtIPTLVLJw35Hk3VK1Pt8puYFQY7aqRumFrTrE+1sHAxNOmNxSaXdyIqDCIyQkQWikiRiNxVTZiLRGSeiMwVkTciaV+42Cn44WGqkJpql04acuvD4p34SF1IROKAZ4GTgWJgqoiMU0rNCwlTANwNHKuU2iIizSNlnyVymKp9sdCKjKWKh05Me45+E8kWw0CgSCm1VCm1DxgNnFMpzHXAs0qpLQBKqQ1+GxULNacYuEXjMH0VU52Yuoqpnrg07oJYj0Q5ksKQA6wK+V7sHgulE9BJRL4VkR9EZERVEYnI9SJSKCKFJSUlYRlTj55R+Bg68c4PDDVLC8b2o2g1KzYLYFMxrfM5HigATgAuAV4SkYzKgZRSLyql+iul+mdnZ0fWwirwo1AyrQA29V2zhYDFop9ICsNqoE3I91z3WCjFwDil1H6l1DJgEY5Q1At0lFG2oGtYmCbwQUx1S1nMIJLCMBUoEJF8EUkARgLjKoV5H6e1gIhk4biWlvpplFb/qMa4TMOfVlHDTTGd+q6zEDe93mFqjjCv78NfIiYMSqky4BZgAjAfGKOUmisiD4rI2W6wCcAmEZkHfAn8Vim1yQ97dI7L11oI+JBpdMZpaovGtJfNMHOMx9TOelN3qfObiA1XBVBKjQfGVzp2X8hnBdzu/sU0OjKRqfsOBzF10pxp6Ox8jgXB0ppesZBgVWBa53PEMdWdYZpZtggPj1jQPtPyqh/9J7HwHEOJWWEw9UGbapfpmJpuphWaWmfpa4zLD0wb5mtaXqiJmBUGS/Qxt7Vmd/6KFoZmiZjopwvFCoNGjC3oNDattazj78OboWXHO9OLczOzlxZ86Q8zrPM5iKHFRAViXhhMLeh0EAsjKky1SyexUDjpNMdUga9PeTVmhcH0Z2QnIDUMjHuOhpdOhpsXM8SsMJiKqbUdSx2JoRJOq6vSMB0NYqhZvhHzwmBqRtRKLNyjpV5jqoyaapffxK4wxFCNTiemjtgxzmXjA6beoamtXFMrffUhr8auMPiAsY9by3tr5ssfC8s1+yOkDRdzn6OhhlWBFQYN1J/HbYk0ptVabV611IaYFwZTm3WmFSiWumEL4HAxNOPH2AsZs8Jg6ovrSzM4tvJ0g8XUCZQ6MHbSo6l+KZ+JWWGIBfzwaTbs/X31x6UD0ye4mdbqjs2iXC9WGCy1wtRZ1Frj0hdVOWYVmbrT3hbB4WBaxaMqrDDUg4dkqX/YMrNhoWfpHA2RRIiYFQbzm+cWS+xhWm26HpXlWolZYdCKdWdYIoBphaZO7KRHs4h5YWjIWcf8VpFZy4EfisusXOHPxCiz7lErsVrN10jMCkN9moVoAqbu/KU1LtM3ydaAzffRx6wcUTUxKwwWi5/EVAEcA31rejaCqj9YYTAUnZOZDKu0WhoIxrfWNGCqXX4T88Kgt9DUMtXSexz6ozKfGFA/UycXmkos3KNfRFQYRGSEiCwUkSIRuauK81eJSImIzHD/rvXPFo1x6YuqHmBWS8Z08TNt/Lvp6aWDmHLj+UR8pC4kInHAs8DJQDEwVUTGKaXmVQr6llLqlkjZZakdsTRjNhYEy2KpiUi2GAYCRUqppUqpfcBo4JwIXr9KdAxNtC3WhoHpghULmLpQoN7tS828x1AiKQw5wKqQ78XuscqcLyKzROQdEWlTVUQicr2IFIpIYUlJiR+2Rp2GPC7fEn1MK5tM1WSdbilT77EqTOt8/hDIU0r1BCYCo6oKpJR6USnVXynVPzs7O6wL+fOMvMeqd4x/PcqJHjGsnPMFU4dMmpr2ptpVH4ikMKwGQlsAue6xcpRSm5RSe92vLwP9ImSbJhp+VjSupmm4+JmWXjoxtQZsql31iUgKw1SgQETyRSQBGAmMCw0gIq1Cvp4NzPfbKFNrYbFAgy40o22ARSsNOa9WRcRGJSmlykTkFmACEAf8Wyk1V0QeBAqVUuOAX4nI2UAZsBm4yi97bK0iPHSkm0368IiFPGta+evLemP6o9ROxIQBQCk1Hhhf6dh9IZ/vBu6OpE2xQKzVdhoqpi5gqANfXIJm3SL1qUpkWuezRSOmrq7qx/tqxc8SpP4Uv+YS88Jgy5PoocUtZUsBi0U7MSsMpo9mMQ0/0ktL68MHZY+FyoLO52kXfGx4xKww+IHN1LXDVEk21S4/0NHHoLW1pjEu+xp6J+aFQUdtx4+lFOz+0XUjFlxKxhXmlvCoBy9k7AqDoS+I6S+uqXna1NaansLc8ExhGDa1vBO7wmCpE6aWTdYuSyTQWvGoB3nDCoMlahhaybdEEZsnzCDmhcFUF4ROjFvm1/Aak2nJ5Qem3aPhWSLmiFlhMH+VSeubjjiGTgjUGZcvw461x6gH02Z3l2OoWaHErDDoxNBRe75gWk0ziGmtIp2ibGqeMHUukKkVIkPNqhIrDJZaYWqmNtQsSx0xS9YPx3T7dGOFwWKJQWKtoAsXUytEfhOzwmBqc9MPTC0ETHP/BDHWN60DQ7O9oWbFLDErDKbTkDcQMrUD1FS7LOFhaL2jXuSJmBcG0zKP6S0Z02rTpqeXDmLgFrVianoZalaVxKww+DJc1awyUyumjkCxhIdpw2gPxWXmS2SqXX4Rs8KgE1NrKBZLZbQOrdYYmaktv1itEFlhiAUMrewYalaDbvnpxPR0Mtw8o4l5YTDNZx7ETKv0YOqWo6badSguM3OFaZV9w8ypl8SsMJiWmYMYalY5hpZNxqHVzaIvKosBmCrwocSsMMQUOkoWWzo1KExtKZtGLPSjVEVEhUFERojIQhEpEpG7agh3vogoEenvt016d0oz9GUz1CxL5DG1cDLTqtglYsIgInHAs8BpQDfgEhHpVkW4dOBWYIq/9miMy9BsbWgZcIgYECydlQVTk8tUz0h9cNmYSp2FQURS3UK+rgwEipRSS5VS+4DRwDlVhPsT8CiwJ4xrRAU/WgoNOU+bv+S5d3RWFoyt5ZtplrF21SeOKAwiEhCRS0XkYxHZACwA1orIPBF5XEQ61vJaOcCqkO/F7rHQa/UF2iilPj6CTdeLSKGIFJaUlNTy8v6jozAwPVMbVwDbTt7wMO1BuphaIdLrcjaf2rQYvgQ6AHcDLZVSbZRSzYEhwA/AoyJyuVdDRCQAPAnccaSwSqkXlVL9lVL9s7OzPV23PjwkE4ipQrMBY59j3YjVikd8LcIMV0rtr3xQKbUZGAuMFZFGtYhnNdAm5HuueyxIOtAdmOQ2nVsC40TkbKVUYS3irxOm9gv4gbGd4haLi+kt5VjjiMIQFAURSQKCbqMipdSeymGOwFSgQETycQRhJHBpSBxbgazgdxGZBPzGD1HwC9MKYJ3vmll3Vo8wdE2iWMAmV/jUpo8hXkQew+kTGAX8F1glIo+ISG1aHAAopcqAW4AJwHxgjFJqrog8KCJnh2e+GfiyXHMMlAJaR+xoTC8dcdkacDSxie+V2vQxPA40BfKBj5RSfXH6HLKAv9blYkqp8UqpTkqpDkqpv7jH7lNKjasi7AmRaC3EQgFsGrEwVNgPdKabzlyvNy4z30dT7fKL2gjDmcD1SqntwFkASqltwC/cc/USU2t0phZ0ZlrlD6YOD9WBqbdmbr73wxugPUrt1EYYlDpUrZaQgweAg75YFcPYORENC5v0liCminJV1EYY5ovIz93Pa4IH3SGq832xKoKY+uLqqLWaWvONBaGyG0FFH5te4VObzuObgfdE5Bpgmog8AfQHkoBz/TTOYh6x0Cej4x51ppI/y4E33Odo+vLp9YHaDFctBgaIyDCcNY4APlZKfeGrZfWQhrz2vs7Wh94VK82MqzxO/VE2aAzL9vXK/aOTIwqDiIhy+Bz4vKYw2q2rJ9hRNpb6hmluRsPMKcePUq0+jHCq1ZIYIvJLEWkbelBEEkTkJBEZBVzpj3n+Ewty1pDv0fR7M9w8LZhaJzTNnVefKn216WMYAVwDvOnOWi4FknFE5TPgKaXUdN8s9AnTakx+0PDv0Fz8WUHWtEl8Noc1VGrTx7AHeA54zl0TKQvYrZQq9dk2SwPH1H2aTUPnLZqeXoY2Poy1yy9q08fwJDDL/ZurlFrru1UW41wQWgsnw9sysVYImIAfOULLjrZaZ5vXn4xVG1dSETAYuA7oKiLrOCQUU4GvlVJ7/TPRb3Q2zzVE4sdQO/1RNkjsmlcWi0NtXEnPhX53+xl6AD2BG4EXRORGpdQEf0z0B7PrrHow3W3QkIkLOIl/QOPaAFZj6oZNrvCp9eqoQZRSy4BlwDgAEWkFfISzaqrFEhVMKzTjXWEoO6hBGXTOIXGrRAcNSy+dmLp1rOku1FDqvOdzZdw+hzc02BIVTCtQTMe09DL1VYuLc16tMsNK4ECwJaNzmXJtMZnnh3eTi4OmZXyf8SwMAEqpJ3TEE0lMd7OYlg99WWZAf5Ra0GFXo2ABfMCsu4xzH+RBDYKlM0sEBUtHA0snAY3pVZ/QIgwWffiz+JpZmdpUUdZpV1y5K8mstHcbMhwwzi79LRkdBIXBNLv8xgqDRkxrBpvraGn4xMcFO5/1VYF15K7yGrBhBV15AayjJePGpXMnPp0tGcOSvkpiXhi0dCqZWgWOIUwT5biAvj4GnbnrkGCZll76BMuPfgE9gqXBkAgRs8JQn0YImEAwtUyracaVF3RRNqQS8QEzC+Bgzdw4F5fGFsOh1ofnqMqxriSLEZhWAw74UNDpeNcOFcAaXTYa7DK3j8FQV5LGvo/ye9SY9rbzOcYw7P3wZX8BHXlaZw1YbyevoS6boDAY1pTRWTMvR8eaVwT7BbzH5UdHtm0xxAj1yd8XLvEaa4fBAti0F8SXgk4DAY0thmBe1dGZqnNYqB/vkI78Vd7HoDFPmJa//CaiwiAiI0RkoYgUichdVZy/QURmi8gMEflGRLpVFY+ldugsnEz1mZvamRosM/draDEcmnvgOSrjh1/qLMx1uvF02mVo0lcgYsIgInHAs8BpOFuEXlJFwf+GUqqHUqo38BjwZKTs04FpDzxeo6+13Gdu2oQtQwUryM69BzzH4Ufrw9T00lmY67xHU4XULyLZYhgIFCmlliql9gGjgXNCAyiltoV8TcXHybGmjrIpR4evVeMIFD86LXV0sMcb2skbZMfeMs9xxPkw98DUfK+zMNeaXmZ1FflOnRfR80AOsCrkezEwqHIgEbkZuB1IAE7yy5hGwbVsNNaATdv+T+foDL0FsP571DpaSmN9ZPue/Z7j8OMeTWv5BTFt7kF5XIYKqV8Y1/mslHpWKdUB+B1wb1VhROR6ESkUkcKSkpKwrtMo3rn1fQe8N/VNReeQSVNdNofu0cze1O17vLcY/Fj4zrYY6oZp+d5vIikMq4E2Id9z3WPVMRr4WVUnlFIvKqX6K6X6Z2dnh2VMgtti2FfWcB94nMYx/ub3MUTZkGrYW+bdMJ19RUF0ut50trBsYW4GkRSGqUCBiOSLSAIwEndPhyAiUhDy9QxgsV/GJMQ7L9s+U0sUDegcSWTqImd+THAzDT9mK+8t895S9mP1AK3uH8OEoT4NkY9YH4NSqkxEbsHZ0CcO+LdSaq6IPAgUKqXGAbeIyHBgP7AFuNIve4J9DPs11OiC6CwztSyYptOVJGYWwDonuJmKHzN59+73/hz9mJ2vdYhpw80SvhPJzmeUUuOB8ZWO3Rfy+dZI2ZJQ3sfg/QUJaKwJ6PTla53gFqd/yKSOF7eRa9d+jS4u3Y2isgMHiY8Lv3FevlS2RsP27Dezb22fxoqaaS2G+oRxnc+RorzFoEUYNK4MqdFl48cENx19DDo3iwk+xx0aRv8EBV73/hWbd+3z9HudS1IH2aOxANbpUtLRJxPENLdnfSJmhSE90WksbdutoUApr5l7joo4jdP5GwX0DclNddNrp45x+T6McNIx+ifOp1nBm3Z4EwY/5pDorJmbimnrVAUxbeOsqohZYWiWlkhAoGT7Xs9xBWuaWjt5NcSVluQU5jomWQVr5lqGX/owYUvrsFDN5YlXYfCjxRALmJZc5RsIRdmO2hCzwhAXEDJTE9mgRRj07Rh1qND0HFV5LX/rLu+toiDbtEzYcv7rLOj02OXPktSbdnrPY2Du7FvTlog3laDDrT4IfMwKA0DrjCRWl+72HE9AY2eq1v4K166tGtxlQXS0PnS63oJodSVpfnE3emwxBDHNZ243uwoPUycXhhLTwtAhO42iDTs8xyMaC3M//O9bPHZ+hmKqK0mnYOkWhg3b92iJx1SfuaVu1IMGQ2wLQ8fmaazdusdzoaJzj9mAD+6MtVv1FEwApRpExtQ9FOI0PsfQGFZv8d4qBdi1z/sQ03pQWW3wmJbvqyLmhQHw3Go41Mfg2aTywkln5inesktbXDrcIn5te+l1xJQfrbW0xHiKDRIGS/SpD9uExrQwdM9pAsDMVaWe4vFlHoPmGaA6hyd67WT3YyYveB9hVr5LnUa7cpsmaxPmnfu8u8tCqQ8FVLTxo4Vl+xgMJycjmVZNkihcscVTPDpn8vrlZlmn0Z20zWM/g1/rLnkdYebHaKncpsls3LGP3Rpq+7pbDDs0C42ldpg2iKAqYloYAPq1a0rh8s2easE6hzmWb1WpOfMs37RTW1xea+YBjTOfQ5PJayevH9te5jZNAfS483RPSvM6kKC8QqS5T9zUCWC67DJ12HEoMS8MQzpmsXbrHuav3R52HL7MY9DcYli0Pvz7q4x3l43zX3eraI3Hocd+uLg6ZKcCsFjD6DfdeN1EKDhYVfc8Bp3LYugUGV19YtaVVA84uVsLAgKfzlkbdhwBjTUnvzbE0SoMO/S0GPS3irzVyg+5uHRY49CxeToBgQXr9KW/Lry3GPQNugjF6yCCUHv2aFhFNoiuFpt1JdUDmqUlcnSHZoz9aXXY48R1zmM4VGh6jqqcjs3TWLReX43Vq1vEj5p5akIcKzy6y/xorSU1CpCXlcqCtduOHDjCeG0x6JzYGcrOvfr6UnRsrRpEV0umHuiCFQaAK4/OY3Xpbj6Zsy6s3+t8QcqXA9fYnO7cMp2F67ZrmyC1arOumrm+N6Rds1SWb9Rkl+aSrkvLdG0tBh2rAQfx2mLQ6UINZftes5ZwCeL1nawHelCOFQZgeNcWdGyexl8/WxjWOvU6X5A0jauYBunbtim79x/w1I8SJDs9kVWbvfnyE13x09nMz8tKYc3W3Z72GTi0U5re3sFeuRms3LxLywzoUo3rXnldKuXQaDy9Rd4ODbPrg2zdrS+u3YbuYeEHVhhw5g7cf9ZRrNi0i79NXFT332v0mackxAF6lngIMiCvKQA/Lt/sOa62mSms9NhiCLaKdIpft1aNUQrmeXDZBFt+Ol0ZAAPzMwGYuszbsGjQu7yJ18mKAZ9WC9WZ93W2GHTmV9OxwuAypCCLSwe15YWvl/Le9OI6/VbrPAa3dNKxJlGQlo2TaJOZzA9LN3mOq21mCqtLd3tySwUXX9NZAPTIzQBgdvFWz3HpnkjWPacJKQlxTFnmPf237NQnDCUeWzA6830oOvOFzvdIp12mY4UhhD+e1Y1B+ZncMWYmb0xZWevf+eFr1V07OaFTcyYvLvE80ap9VioHDiot8yJ0i19WWiIzi0s9x6XTlQHOXhaD8jP5YsEGz3lEZ4thwzZvo8vErz4GjemvYyOuILtiaEKgFYYQEuPj+M/VAxlSkM09783mttHT2VSLoZk6t70MonOpbIDTurdkz/6DfLVog6d4urVuDMDcNd5H2egUPxHo1y6DKUu9TVYE2O5DzfC07q0o3rKb2au9tWh0LeHdKE48zxQ/tBWqBoNC0JkvdLawdmh2MZqMFYZKJCfE8cpVA/j18E58OGstJ/x1Es9+WVTjqqIpiU6/gM4ahde5ApUZmJ9JVloiYwrr5iarTIfsNBLiAp58+UG2aOxIBTihc3NWl+72PJlMd4sB4JSjWhAfEMbNWOMpHq+T+II0T0/SNlNcd+ezznyxXtOS52D7GGKeuIBw6/ACPr31OPq3a8rjExYy+OHP+c3bM5m0cMNhQwZTE9yRRBrXslm/TV+GBoiPC3DZoLZ8sWADS0rCLzjj44SCFmnM09Bi2KhZ/E7onA3AxHnrPcXjx+iTjJQETj2qJWMKV3kqYHRsLAXOJlUl2/eytyz8e/VrVJLOfKFzjTArDBYAClqk88rVA/n0tuP4We8cPp2zjqtemUq/P03k+v8W8sq3y5i/dhuJjZwWg86Ms31PmXbf7eWD25EYHwhr5FUovdpkMGNlqZZ5ETrvsVWTZAbmZ/LW1FWeJ6n5sV7PNUPy2LanjLemrgrr9/EB0baEd35WKgcVrPQwWzw4iEB357NWYdBYwbKdzz4hIiNEZKGIFInIXVWcv11E5onILBH5XETaRdK+6ujSsjGPnN+TwnuH8/LP+zOie0vmr9vGAx/O47SnJ/PM54sB/cMcdfczZKcn8ovjO/DRrLV8V7Qx7HiObt+M7XvLmKOh1bBN4zhzgMsGtWXl5l18scBbX4rutAdnPsnR7Zvx9y8Wh7UPd47GJbzzs5y9SJaUhD+IINjHoHu8qi5hSE+MZ91WfSJjWww+ICJxwLPAaUA34BIR6VYp2HSgv1KqJ/AO8Fik7KsNSY3iGN6tBY9d0IvJd57E5DtP5IkLe3Hl0Y5+6Z4YpbO2E+SG49uTn5XKbW/NCHsxvMHtmwHw3ZLwxSWI7ns8rXsr2jVL4fEJCz3NYNa5610QEeEPZ3ajdPd+Hho/v86/z89KZf22vVrG5udnOYv7Ld0YvlvRj90G05Pi2aSpgz2naTIbd+zV1vfnR2XBVCLZYhgIFCmlliql9gGjgXNCAyilvlRKBatEPwC5EbSvzrTJTOH8frk8cE53urRM17aFY5AVHheFq4qUhHievbQvW3fv58p//xjWVp3Z6Ykc1bqxZ18+wJqtetMsIT7AXSO6sHD9dv7xRVHY8Xhd9qM6urVuzI3Hd+CtwlW8P311nX57lDsiTEf/TnpSPK2aJLHAw2z4YINBpzBkpyWyacc+La684A6NSzboWXJ+s8YRTqYTSWHIAUKdq8Xuser4P+CTqk6IyPUiUigihSUlJRpNDJ92zVJYobkwWeqhmV8T3Vo35qWf96eoZAfn//O7sLY2PbNna6avLPVcgC7fqP8eT+vRinP75PD054v4eFZ4q+b6uUz27Sd3YmBeJr99ZyZfLap9/u3WytlxUMdQYYDebTKY4WH3QvFh5nN2eiL7Dhxkk4ZCuKB5OgCLN+hZp6pEU0umPmBk57OIXA70Bx6v6rxS6kWlVH+lVP/s7OzIGlcNHbLTWL5xp5adusAZZ77MQzP/SAztlM1/rxlI6a79nPX3b/jnpCV1WiTsrF6tAHhnmrfhr36J31/O7U6/dk25dfR0Xv1hRZ1roDqXKa9MfFyAl67sT0HzdK4bVVjrNMxOT3R2HNSwtAk4wrBy865azdWpCj9WV22b6Wxs5HXZFXAqa43ihIWanmW46VQfiaQwrAbahHzPdY9VQESGA78HzlZK1Zsn0T+vKWUHFdNXeV8PBxwf8DIfatOhDG7fjI9/dRxDCrJ49NMFnPjXSfzn22W16mTLbZrC8K7NefWHFWGLYeOkeN/uMSUhnn9fNYAhBVn84f05XP/qtFq3brLTEz25WGpDk+RGvHHdIAbkN+U3b8/k9jEzajUZ67iCLL4p2qhlRFi/ds4aWj8sDU9odM58Dm72066ZKwwa3KjxcULXVo2ZvrLUc1xZaQna+j7A/P22IykMU4ECEckXkQRgJDAuNICI9AFewBEFb8NKIky/dpkEBCYv9t4hC04zeMG67b5noJZNknjp5/35z9UDaNUkifs/nEf/P/+P20ZP5/P562vsuPvF8R3YvHMfr3y3LKxrt89OY6mHORVHIj2pEf++cgB3ndaFbxZvZNgTX3HnOzNZsK5mV0z/dk1ZuH677z7ljJQE/nP1QH55UkfGzVjDSU9M4vmvltSY5id0bs72PWVaFkTs3SaDjJRGfD7fW1+Rzj6GNm6LQVf/2oC8TGasKvU0XwOgWWoiu/cf0DYyyfT9tiMmDEqpMuAWYAIwHxijlJorIg+KyNlusMeBNOBtEZkhIuOqic44miQ3YminbD6YvlrLev692jRh+54ylvrcaghyQufmvHPjMYy98RjO7ZvDFws28H+jCun9wEQufekHPq1ir4oBeZmc3K0Ff/+8KKxhlF1bpbNm6x5fm+iBgHDD8R34/I7juWhALuNmrmHEU5MZ8dTXPPtlEXNWbz1MfI/poG/U1ZFoFBfgjlM6M+6WIfTIzeCRTxZw9MNfcP+4ucxds/Ww2viJnZuTnhQf9lyIUOLjApzUuTlfLNzgaa8BnVM+khrF0bJxkudNl4IMzM9kX9lBZq7ythRJiyZJAKzVNFginOHKkSSifQxKqfFKqU5KqQ5Kqb+4x+5TSo1zPw9XSrVQSvV2/86uOUazGDmgDWu27uHdn7z53QH6tHWa+V46B8OhX7umPHRuD6beO5zX/m8QVx2bx+ad+/hiwQbiAkKKO8s7yB/P6kZA4JY3pte5Vta7TQYQmXtsnZHMn3/Wg+/uGsb9Z3UjNTGexycs5My/f0PfP0/khlen8fFsp6O6Z24GWWkJfDjT2/IVdaFb68b895qBjL3xGI7vlM0bU1ZyxjPfMPTxL3ngw7nl4ZIT4jivTw6fzF6nZRb02b1bU7prPxPmhrdJFeif4NapZTrzNW1sdHSHZiTEBTzdH0BeM70tGdOHvhrZ+VxfOfWolvRpm8GfPppHkceREB2y02icFM8UDUtlh0NifBxDCrK45/SufHrbUKb/4WQ+v/14MlMTKoTLbZrCExf1ZsaqUn715vQ61Ty75zQhLiBafMC1JTM1gauOzWfsjccw5Z5h/O3iXpzctQVz1mzlA3cdo4yURpzXN5fP52/QtgRFbenXrinPXNKHKfcM4y/ndqegeXr5tqzpSY4oX398BxB4YsJCz9cbWpBN28wU/vPd8rD7CnTPEu+R05jF67d72nQpSOOkRgztlMUns9d6csu21ezi0rnhkh9YYdCIiPDMyD4kxAc477nveG96cdiZMSCOe+eLBRu0bzUZDk1TE8hzJ0VVZkT3lvzxrG5MmLueq//zY61nrqYkxNMjpwmTF0dnyHGLxkmc2yeXxy/sxTe/O4nCe4fz6W3H0a5ZKlcdk0dcQHg4jIloOmiamsBlg9rx76sGMO/BU5n466F0beXMY8jJSOa64/J5d/pqPvNYEw4EhOuOy2faii18Pj+8bj3dq4f0yGlC2UGlbTvUs3q1Zs3WPXUaGlyZzNQE0hPjtbm4SnebPfTVCoNm2mSm8N5Nx5Kflcqv35rJiU9M4un/LWb6yi11HkkyvFsLNu3cx7QVekY6+cnVx+bz+AU9KVy+hdOenlxrUTy5WwtmFm/VvmhgOGSlJdKlpVP4ts5I5uYTO/LRrLW8+v3yqNqVkhBPQYv0CsduHdaJ7jmNuePtmZ4nvI0c2Jb22ak88NFctocxq1r3Ino93U2XdA3LPb1HK1o2TuKFr5eEHYcItM9OLW+9ecW2GGKQoDj8/ZI+tG6SzN/+t4hzn/uOXg98xnnPfcvd787mlW+XMXHeeuat2cbW3furbI4P69Kc9MR4Xp+yIgp3UXcu7N+G928+llZNkvj1WzM58+/fMHZacY19D6ce1RKAD2bUbRZwJLj5xI6c2DmbP3wwl+cmFRk1xDAhPsDzl/cjLTGeK/41hekrw688NIoL8Nj5PVlTuoc735lV5xaqji1tQ2mdkUyH7FS+1jTCr1FcgOuHtueHpZv5n4fZ+j1ym1Q5WCEcwl2OJlJYYfCJQEA4q1dr3rx+MIX3Ducfl/bhgn65NIoLMH72Wh74cB7X/beQ05+ZTK8HPqPH/Z9x0l8n8a9vDg39TE2M54L+uYyfvda3JRp007VVY96/6VieuLAX+w8c5I63ZzLgz//jjjEzmThv/WHr/HRsnsag/ExGfbdCy9h8ncQFhH9e3o8zerbisU8XMvKlH5gZ4cEANZHbNIXXrx1ESmIcF7/4A//9fnnYhVb/vEzuGtGFT+as4553Z9fpWfjh6hzaKZspSzdpmzB6xdHtKGiexn0fzAl7856euRls36tnpKCufTX8Iv7IQSxeyUpL5MyerTmzZ2vA6azbuGMfq0t3s6Z0N6u37GZ16W5Kduxl4/a9dG6RTnpSIwCuO649b/64koc/mc9zl/WL5m3UmkBAOL9fLuf1zeGboo28P30Nn81bx9ifignI4aNY/m9IPte/Oo3RU1dx+WAjFtQtJ6lRHP+4pA/HF2Tz8CfzOefZbzmuIIuLB7RheNcWJLlLrkeL9tlpfHDzEG57awb3fTCXcTPW8LvTujAgL7POcV03tD3b9+znmS+KWLVlF3+7uDctGicd8Xd79+sX9JO7teCVb5fz2bx1nNO7ppVzakejuAB/vbAXFz7/Pbe8+RP/unJAnZ9d37YZAPy4bHP5OkzhEulBDXXFCkMUEBGy0xPJTk8sH7JZHa0zkrnphI48OXER709fzc/6eH9JIoWIcFxBNscVZLOvrAeFKzYzZelmJi8uYdueMrLTEwGnEDi6fTP++tlCTurSnNYZyVG2vCIiwkUD2nB6z1aM+m45b0xZyS1vTCclIY5jO2ZxYufmDMzPpH1WavmKo5EkMzWBUVcP4J1pxTz66QIufP57junQjMsHt2N41xYkxNfeMXD7KZ1pk5nC79+fw7AnvuKXJ3Xk8sHtSE2svqjwOnmsKgbnNyO3aTJvTV2lRRjA2UfkofN68Ju3Z3LtqEKevawvTZIb1fr3HbLTyG2azBcLNnDpoLaebLEtBotnbjqhA98s3shd786ieeNEjumQFW2T6kxCfIBjOmRxTIcsfn1ypwrnRIS/nNuds//xLTe+No3XrxtMWg0FUbRIS4zn5hM7csPxHfhuyUYmzF3HlwtKyleZbZwUT++2TemR09iXWnRNiAgX9m/DmT1b89oPK/j3t8u46fWfaJaawMndWjC8awuO7Vi7fHNh/zYMyMvkgQ/n8vAnC3hu0hIu7JfL2b1b0yOnSflSGEH2epgcVx2BgHDJwLY8PmEhM1eV0usIFajackE/Z8Hm342dxelPT+aR83twXEHt1lsTEYZ1ac5bhavYta/ssDk9taVZagKrS3ez/8BBGsWZ6c037+2zHEZ8XIDnLu/LJS/+wDX/mcpjF/Ti7F6to22WVtpnp/G3i3tzw2vTuOJfU3jp5/3JSkuMtllVEhc41BJSSrGkZAc/rShl+qpSpq/cwvNFG8v97skJkXU1JSfEcd3Q9lwzJJ+vF5fwzrRiPpq1ltFTV5EQHygfj38k8rJSeeXqgUxfuYWXJi9l1PfLefmbZeRkJHNsx2Yc0yGLRLclomO+QVX8/Oh2vDx5KY98soA3rht0mCCFywX9cumQ7exJcsW/fuS4giyuO649QzpmHbHFd1av1oz6fgXjZqxh5MDwWg1dWzXmm6KNFG3YUT4E2TSsMNQTstISefP6wfzi1Wn86s3pTFqwgbtO60LzWviA6wsnd2vBs5f25Vejp3Pa05N5+NweDOvaXFuB4AciQsfm6XRsns5FA5w1IveWHWDZxp1s2bmfzpWGmUaKuIBwYufmnNi5OfvKDjJl2Sa+WljCj8s3k54UX2t3XZ+2TXnusn5s3bWfT+eu5fP5G/h0zjrGFB6a3e/XYK30pEbcfkpn/vD+HF6bspIrNPY/9WnblAm3DeW1H1bwz0lL+Pm/f6RNZjKndmvJsK4t6Nsug8T4w0W9X7umdG6RzqjvV3BR/zZhuQ6Pau0Iw9w126wwWLyTlZbI6OsH89T/FvHi10uZMHcdIwe25apj8soXH6vvjOjekg9uPpZfvTmda/9byJCOWdx0QgeO7tDMaIEIJTE+rnw+hAkkxAfKWzjh0iSlERcPaMvFA9py4KBi/tptzFhVyr3vz/G1cLt8UFsmzlvPgx/OJa9Ziqd7qExSoziuPa49Vxzdjk/nrOPdn1bz3+9X8PI3y0iIC9A9pzGZqRVbrSLC9UPbc8fbM/lo9tqwWu55WamkJcbz08ot5a4t0zDTwWWplkZxAX57ahc++/XxDO/WglHfLWfo419y8Qvf8+oPK4wf7VAburZqzMe/Oo4/ntWN+Wu3cenLUzjlb1/z988Xe15qxOKduIDQPacJlw9uxwmds4n3scNdRPj7JX3okJ3GtaMKGefD+lWJ8XGc0zuHUdcM5Kf7TuaFK/px1bF5BET4n7vybEbKoaVgftYnh66tGvPQx/PD2gExToRjOjTjq4Ul2pcT0YVtMdRT8rNSeXpkH343ogtjClfx4cw1/OH9OfwBZ037o9s3Y0c93rw8IT7A1cfmc8nAto6P/MeVPDFxEU9MXERu02QG5TdjUPvMsMekW/TQJLmRb5sthV7j9WsHccNrjhv1ywUbuOf0ruWj2nSSlhjPqUe1LJ94uf/AQTbu2EvLEJdtXEB47PyenPfPb/nN2zN5/vJ+xNexE/nELs35bN565q7ZRvecJlrvQQdWGOo5rTOSuW14J24dVsDiDTv4ZvFGvl+6ifGz17JtTxkBoUpfaX0hqVEcF/TL5YJ+uazftofP5q7j26JNfLlwA2NDVrGNwihRC85Q2UiIc7O0RF67dhDPflHEP79awvjZa7mofxsuGdiWrq3SfXMzNooL0KrJ4f0xPXKbcO8Z3fjjuLn8buxsHj2/R53E4bTuLbl/3Fxen7KSh8/rodNkLVhhaCCICJ1apNOpRTrXDMnn4EHF8k072bXvAE1Saj9W22RaNE7iiqPzuOLoPA4eVCzesIOZq0rZsmsf7bO8TTiyhEdmSgLb95axr+xgneZLhENifBy3n9KZc/vm8sJXSxg9dSWv/rCCguZpDOvagmM6NAtrYl+4XHlMHlt27eOp/y1m/bY9PDWyd61H0mWkJHBe3xzG/lTMjcd3oG0zs/oIrTA0UAIBoX12wy0sAwGhc8t0OreMzqgfi0NmmuN737JrX61mSesgPyuVR87vyZ0jujB+9lo+nLmGlycv5fmvlhAfkIgOxLhteCdaZyRz73vOhMA7R3Tmwn5taiWStw7rxAcz1nDPe7MZdc1A4gxq9trOZ4vFEjbZbg05GqvjZqYmcPngdrz1i6OZdf8pjLpmINcPbU9u02TymqV4XraitlzUvw3jbx1C5xbp/P69OZzw+Je8+PUSNhwhTVo2SeK+M7vxTdFG7vtgjlGLNNoWg8ViCZt2zZw9OlZs2lW+XHY0SEmI5/hO2RzfSd9w1rrQsXk6b/1iMJMWlfDcl0U8NH4Bj3yygEH5zRjaKZvjCrKqXH5j5MC2LN+0i+e/WsK6rXt4+PweNE+P/twkKwwWiyVsDu1sFpm9yU1G5NCkwiUlO3h/+momzlvPo58u4NFPIbmaRfvuOq0LLRsn8tAnCxj2xFdcc2w+Vx2TR9NKuyVGEisMFoslbJIT4mjZOIllG+vHsvCRokN2Gnec0pk7TunM+m17+LZoIz+t3MKSDTurXPfpqmPzGVKQzWOfLuDpzxfz3KQiTurSnNN7tGJIxyyaRXh5GCsMFovFE51apjN3zdZom2EsLRoncV7fXM7rW/Ms547N03jx5/1ZuG47bxeu4v0Za5gw15lg161VY/q0zaBnbhN65GRQ0CLN1wX4rDBYLBZP9G6TwT++WMzOvWU1Ls9tqR2dW6Zz75nduPv0rsxZvZXJi0v4bskmxs1Yw+tTVgIQHxDys1K5bXgnzujZSrsNEX2KIjICeBqIA15WSj1S6fxQ4CmgJzBSKfVOJO2zWCx1p0+bDA4qmFlcWi+XhDeVuIDQq00GvdpkcMtJBRw8qFixeReziktZuG47izfsIC3JnyI8YsIgInHAs8DJQDEwVUTGKaXmhQRbCVwF/CZSdlksFm/0bdeU+IDw1aISKww+EnBbCflZqf5fy/crHGIgUKSUWqqU2geMBs4JDaCUWq6UmgWYtfmvxWKplibJjRjcvln5hkWW+k8khSEHWBXyvdg9VmdE5HoRKRSRwpKSEi3GWSyW8BnRvSVLS3YyfeWWaJti0UC9nPmslHpRKdVfKdU/Ozs6E1osFsshftYnh7TEeF75dnm0TbFoIJLCsBpoE/I91z1msVjqOWmJ8Vw2qC0fzlrDnNV26Gp9J5LCMBUoEJF8EUkARgLjInh9i8XiIzed2JHMlATufnc2e8v82QfaEhkiJgxKqTLgFmACMB8Yo5SaKyIPisjZACIyQESKgQuBF0RkbqTss1gs3miS3IiHzuvB7NVb+eMHc43dncxyZCI6j0EpNR4YX+nYfSGfp+K4mCwWSz3k1KNacsuJHfnHl0Ukxge476yjjFpO2lI77DRFi8WilTtO6cTesgO8NHkZyzft4qmLe0d1QThL3amXo5IsFou5iAj3nN6Vh87twfdLNjHsya8YO63YupbqEVYYLBaLdkSESwe15YNbjqVdsxTueHsmpz/zDZ/OWWvUhjSWqrHCYLFYfKNrq8aMveEYnryoF3v3H+CG135i6ONf8vfPF7N26+5om2epBtvHYLFYfCUQEM7rm8s5vXP4dM463vhxBU9MXMQTExfRp20Gp3RrSZvM5GibaQnBCoPFYokIcQHhjJ6tOKNnK1Zs2sm4GWv4zN3hzGIWVhgsFkvEadcslV8OK+CXwwpYXbqbLxdsYOG67QzMy4y2aRasMFgsliiTk5HM5YPbRdsMSwi289lisVgsFbDCYLFYLJYKWGGwWCwWSwWsMFgsFoulAlYYLBaLxVIBKwwWi8ViqYAVBovFYrFUwAqDxWKxWCog9X0pXBEpAVaE+fMsYKNGc3Rh7aob1q66Y6pt1q664cWudkqp7KpO1Hth8IKIFCql+kfbjspYu+qGtavumGqbtatu+GWXdSVZLBaLpQJWGCwWi8VSgVgXhhejbUA1WLvqhrWr7phqm7WrbvhiV0z3MVgsFovlcGK9xWCxWCyWSlhhsFgsFksFYlYYRGSEiCwUkSIRuSsC12sjIl+KyDwRmSsit7rHM0Vkoogsdv83dY+LiDzj2jdLRPqGxHWlG36xiFypwbY4EZkuIh+53/NFZIp77bdEJME9nuh+L3LP54XEcbd7fKGInOrVJjfODBF5R0QWiMh8ETnakPT6tfsM54jImyKSFI00E5F/i8gGEZkTckxb+ohIPxGZ7f7mGRERD3Y97j7HWSLynohkHCkdqntHq0vrcOwKOXeHiCgRyTIhvdzjv3TTbK6IPBbR9FJKxdwfEAcsAdoDCcBMoJvP12wF9HU/pwOLgG7AY8Bd7vG7gEfdz6cDnwACDAamuMczgaXu/6bu56YebbsdeAP4yP0+Bhjpfn4euNH9fBPwvPt5JPCW+7mbm4aJQL6btnEa0mwUcK37OQHIiHZ6ATnAMiA5JK2uikaaAUOBvsCckGPa0gf40Q0r7m9P82DXKUC8+/nRELuqTAdqeEerS+tw7HKPtwEm4EyUzTIkvU4E/gckut+bRzK9fCsITf4DjgYmhHy/G7g7wjZ8AJwMLARaucdaAQvdzy8Al4SEX+ievwR4IeR4hXBh2JELfA6cBHzkZuqNIS9xeVq5L8/R7ud4N5xUTr/QcB7saoJTAEul49FOrxxglVswxLtpdmq00gzIq1SgaEkf99yCkOMVwtXVrkrnzgVedz9XmQ5U847WlD/DtQt4B+gFLOeQMEQ1vXAK8+FVhItIesWqKyn4cgcpdo9FBNed0AeYArRQSq11T60DWhzBRt22PwXcCRx0vzcDSpVSZVXEX35t9/xWN7wf6ZkPlACviOPmellEUolyeimlVgN/BVYCa3HSYBpmpBnoS58c97Nu+wCuwalRh2NXTfmzzojIOcBqpdTMSqeinV6dgONcF9BXIjIgTLvCSq9YFYaoISJpwFjgNqXUttBzypH0iI0fFpEzgQ1KqWmRumYdiMdpXv9TKdUH2InjGikn0ukF4Prsz8ERrtZAKjAikjbUlmikz5EQkd8DZcDrBtiSAtwD3BdtW6ogHqdVOhj4LTCmtn0WOohVYViN41cMkuse8xURaYQjCq8rpd51D68XkVbu+VbAhiPYqNP2Y4GzRWQ5MBrHnfQ0kCEi8VXEX35t93wTYJNmm4IUA8VKqSnu93dwhCKa6QUwHFimlCpRSu0H3sVJRxPSDPSlz2r3szb7ROQq4EzgMle0wrFrE9WndV3pgCPwM913IBf4SURahmGX7vQqBt5VDj/itOizwrArvPSqq0+zIfzhqPFSnEwR7Kg5yudrCvBf4KlKxx+nYmfhY+7nM6jY+fWjezwTx/fe1P1bBmRqsO8EDnU+v03Fzqqb3M83U7EjdYz7+SgqdogtRU/n82Sgs/v5fjetoppewCBgLpDiXmsU8MtopRmH+6a1pQ+Hd6ae7sGuEcA8ILtSuCrTgRre0erSOhy7Kp1bzqE+hmin1w3Ag+7nTjhuIolUevlWEJr+hzPqYBFOT/7vI3C9ITjN+lnADPfvdBwf4OfAYpxRCMFMJsCzrn2zgf4hcV0DFLl/V2uy7wQOCUN7N5MXuZkqODIiyf1e5J5vH/L737u2LqSWozFqYVNvoNBNs/fdFzHq6QU8ACwA5gCvui9pxNMMeBOnn2M/Tg3z/3SmD9DfvcclwD+oNBCgjnYV4RRuwbz//JHSgWre0erSOhy7Kp1fziFhiHZ6JQCvufH9BJwUyfSyS2JYLBaLpQKx2sdgsVgslmqwwmCxWCyWClhhsFgsFksFrDBYLBaLpQJWGCwWi8VSASsMFouLiOxw/+eJyKWa476n0vfvdMZvsejECoPFcjh5QJ2EIWRmaXVUEAal1DF1tMliiRhWGCyWw3kEZwGzGeLsvRDn7icw1V2b/xcAInKCiEwWkXE4s3oRkfdFZJq7hv717rFHgGQ3vtfdY8HWibhxz3HX8r84JO5Jcmg/itcjuVaOJbY5Ui3HYolF7gJ+o5Q6E8At4LcqpQaISCLwrYh85obtC3RXSi1zv1+jlNosIsnAVBEZq5S6S0RuUUr1ruJa5+HM8O6FsxbOVBH52j3XB2cJhDXAtzhrMn2j+2YtlsrYFoPFcmROAX4uIjNwlkpvBhS4534MEQWAX4nITOAHnEXNCqiZIcCbSqkDSqn1wFdAcInlH5VSxUqpgzjLSORpuBeL5YjYFoPFcmQE+KVSakKFgyIn4CwHHvp9OM6GO7tEZBLOWknhsjfk8wHs+2qJELbFYLEcznac7VeDTABudJdNR0Q6uZsGVaYJsMUVhS44K20G2R/8fSUmAxe7/RjZONs8/qjlLiyWMLE1EIvlcGYBB1yX0H9w9qjIw1mrX3B2lvtZFb/7FLhBRObjrHz5Q8i5F4FZIvKTUuqykOPv4Wy3OBNn9d07lVLrXGGxWKKCXV3VYrFYLBWwriSLxWKxVMAKg8VisVgqYIXBYrFYLBWwwmCxWCyWClhhsFgsFksFrDBYLBaLpQJWGCwWi8VSgf8HRI1Os1faX3kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_J)  #All classifiers\n",
    "#print(all_J)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\Theta)$\")\n",
    "plt.title(\"Cost function using Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-all Prediction\n",
    "\n",
    "After training the one-vs-all classier, you can now use it to predict the digit contained in a given image. \n",
    "\n",
    "For each input, you should compute the probability that it belongs to each class using the trained logistic regression classifiers.\n",
    "The  *predictOneVsAll* function will pick the class for which the corresponding logistic regression classifier outputs the highest probability and return the class label (1, 2,..., or K) as the assigned class for this example.\n",
    "Complete the code in *predictOneVsAll* use the trained (one-vs-all) classifiers to make predictions. You should see that the training set accuracy is about 91.5%. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOneVsAll(all_theta, X):\n",
    "    \"\"\"\n",
    "    Using all_theta, compute the probability of image X(i) for each class and predict the label\n",
    "    \n",
    "    return a vector of prediction\n",
    "    \"\"\"\n",
    "    #number of training examples\n",
    "    m=len(X)\n",
    "    \n",
    "    # add an extra column of 1´s corresponding to xo=1 (aka intercept term)\n",
    "    X = np.append(np.ones((X.shape[0],1)),X,axis=1)\n",
    "    \n",
    "    predictions = np.dot (X, all_theta.T) # predictions.shape =(5000,10)\n",
    "    #np.argmax returns indices of the max element of the array in a particular axis.\n",
    "    #+1 in order to label 0 as 10. \n",
    "    return np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 50.61043285238623 %\n"
     ]
    }
   ],
   "source": [
    "pred = predictOneVsAll(all_theta, x_val)\n",
    "m = len(y_val)\n",
    "#Check that pred.shape  = (5000,) => rank 1 array. You need to reshape it !!!\n",
    "pred= pred.reshape((len(x_val),1))\n",
    "print(\"Training Set Accuracy:\", sum(np.equal(pred,y_val))[0]/m*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 58.243130724396345 %\n"
     ]
    }
   ],
   "source": [
    "pred = predictOneVsAll(all_theta, x_train)\n",
    "m = len(y_train)\n",
    "#Check that pred.shape  = (5000,) => rank 1 array. You need to reshape it !!!\n",
    "pred= pred.reshape((len(x_train),1))\n",
    "print(\"Training Set Accuracy:\", sum(np.equal(pred,y_train))[0]/m*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PART 2: One-vs-all multi-class classification with pretrained Neural Network - nonlinear classification\n",
    "\n",
    "In Part 1, you implemented multi-class logistic regression to recognize handwritten digits. However, logistic regression cannot propose more complex hypotheses as it is only a linear classifier. \n",
    "\n",
    "In this part, you will implement a NN to recognize handwritten digits using the same training set as before. The NN is able to represent complex non-linear hypotheses (models). \n",
    "\n",
    "For this assignment, you will use parameters from a NN that have been already trained. Your goal is to use the learned NN parameters (weights) for prediction. \n",
    "\n",
    "### Load trained parameters theta (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ex3weights.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/my-conda-env/lib/python3.9/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ex3weights.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7847e1553548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmat2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ex3weights.mat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mTheta1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmat2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Theta1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Theta1 has size 25 x 401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mTheta2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmat2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Theta2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Theta2 has size 10 x 26\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTheta1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my-conda-env/lib/python3.9/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my-conda-env/lib/python3.9/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my-conda-env/lib/python3.9/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my-conda-env/lib/python3.9/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ex3weights.mat'"
     ]
    }
   ],
   "source": [
    "mat2=loadmat(\"ex3weights.mat\")\n",
    "Theta1=mat2[\"Theta1\"] # Theta1 has size 25 x 401\n",
    "Theta2=mat2[\"Theta2\"] # Theta2 has size 10 x 26\n",
    "\n",
    "print(Theta1.shape)\n",
    "\n",
    "print(Theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class prediction with trained NN with one hidden layer (feedforward propagation)\n",
    "\n",
    "Complete the code in function *predict* to return the NN's prediction. You should implement the feedforward computation that computes the NN output for all examples in a vectorized way. Similar to the one-vs-all classification strategy, the NN prediction is the index of the largest output.\n",
    "\n",
    "You should see that the accuracy is about 97.5%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    \"\"\"\n",
    "    Predict the label of an input given a trained neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    #number of training examples\n",
    "    m=len(X)\n",
    "        \n",
    "    # add an extra column of 1´s corresponding to xo=1\n",
    "    X = np.append(np.ones((X.shape[0],1)),X,axis=1)\n",
    "    \n",
    "    #Compute the output of the hidden layer (with sigmoid activation functions)\n",
    "    z1= np.dot(X, Theta1.T)  #Inputs to the hidden layer neurons\n",
    "    a1= sigmoid(z1)  #Outputs  of the hidden layer neurons\n",
    "    \n",
    "    #Add a column of ones\n",
    "    a1 = np.append(np.ones((a1.shape[0],1)),a1,axis=1)\n",
    "    \n",
    "    #Compute the output of the output layer (with sigmoid activation functions)\n",
    "    z2= np.dot(a1,Theta2.T) #Inputs to the output layer neurons\n",
    "    a2= sigmoid(z2)  #Outputs  of the output layer neurons\n",
    "    \n",
    "    return np.argmax(a2,axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-c313b6147684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTheta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Set Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "pred2 = predict(Theta1, Theta2, X)\n",
    "print(\"Training Set Accuracy:\",sum(pred2[:,np.newaxis]==y)[0]/m*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 3: Neural Network training to predict the digit\n",
    "\n",
    "**Objectives**: Implement Backpropagation algorithm for NN training to the task of hand-written digit recognition.\n",
    "\n",
    "In this part the code is complete, you just need to understand it. \n",
    "\n",
    "**NN cost function:** \n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^k [-y_k^{(i)} log(h_\\theta(x^{(i)})_k) - ( 1 -y_k^{(i)} log (1-h_\\theta(x^{(i)})_k)] + \\frac{\\lambda}{2m}[\\sum_{j=1}^{25} \\sum_{k=1}^{400} (\\theta_{j,k}^{(1)})^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} (\\theta_{j,k}^{(2)})^2]$\n",
    "\n",
    "*nnCostFunction* computes the NN cost (above) and the gradients with and without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,Lambda):\n",
    "    \"\"\"\n",
    "    nn_params contains the parameters unrolled into a vector\n",
    "    \n",
    "    compute the cost and gradient of the neural network\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2\n",
    "    Theta1 = nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "    Theta2 = nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    J=0\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    y10 = np.zeros((m,num_labels))\n",
    "    \n",
    "    a1 = sigmoid(X @ Theta1.T)\n",
    "    a1 = np.hstack((np.ones((m,1)), a1)) # hidden layer\n",
    "    a2 = sigmoid(a1 @ Theta2.T) # output layer\n",
    "    \n",
    "    for i in range(0,num_labels):\n",
    "        y10[:,i][:,np.newaxis] = np.where(y==i,1,0)\n",
    "    for j in range(num_labels):\n",
    "        J = J + sum(-y10[:,j] * np.log(a2[:,j]) - (1-y10[:,j])*np.log(1-a2[:,j]))\n",
    "    \n",
    "    cost = 1/m* J\n",
    "    reg_J = cost + Lambda/(2*m) * (np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:,1:]**2))\n",
    "    \n",
    "    # Implement the backpropagation algorithm to compute the gradients\n",
    "    \n",
    "    grad1 = np.zeros((Theta1.shape))\n",
    "    grad2 = np.zeros((Theta2.shape))\n",
    "    \n",
    "    for i in range(m):\n",
    "        xi= X[i,:] # 1 X 401\n",
    "        a1i = a1[i,:] # 1 X 26\n",
    "        a2i =a2[i,:] # 1 X 10\n",
    "        d2 = a2i - y10[i,:]\n",
    "        d1 = Theta2.T @ d2.T * sigmoidGradient(np.hstack((1,xi @ Theta1.T)))\n",
    "        grad1= grad1 + d1[1:][:,np.newaxis] @ xi[:,np.newaxis].T\n",
    "        grad2 = grad2 + d2.T[:,np.newaxis] @ a1i[:,np.newaxis].T\n",
    "        \n",
    "    grad1 = 1/m * grad1\n",
    "    grad2 = 1/m*grad2\n",
    "    \n",
    "    grad1_reg = grad1 + (Lambda/m) * np.hstack((np.zeros((Theta1.shape[0],1)),Theta1[:,1:]))\n",
    "    grad2_reg = grad2 + (Lambda/m) * np.hstack((np.zeros((Theta2.shape[0],1)),Theta2[:,1:]))\n",
    "    \n",
    "    return cost, grad1, grad2, reg_J, grad1_reg, grad2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the gradient of sigmoid function\n",
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    computes the gradient of the sigmoid function\n",
    "    \"\"\"\n",
    "    sigmoid = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    return sigmoid *(1-sigmoid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initialization\n",
    "\n",
    "Select values for $\\theta^{(l)}$ uniformly in the range $[-\\epsilon_{init} , \\epsilon_{init}]$\n",
    "\n",
    "One effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network\n",
    "\n",
    "$\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    \"\"\"\n",
    "    randomly initializes the weights of a layer with L_in incoming connections and L_out outgoing connections.\n",
    "    \"\"\"\n",
    "    \n",
    "    epi = (6**1/2) / (L_in + L_out)**1/2\n",
    "    \n",
    "    W = np.random.rand(L_out,L_in +1) *(2*epi) -epi\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(25*401+260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for shallow NN (input-layer => ONE hidden layer => output layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentnn(X,y,initial_nn_params,alpha,num_iters,Lambda,input_layer_size, hidden_layer_size, num_labels):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    Theta1 = initial_nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "    Theta2 = initial_nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)\n",
    "    \n",
    "    m=len(y)\n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        nn_params = np.append(Theta1.flatten(),Theta2.flatten())\n",
    "        cost, grad1, grad2 = nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,Lambda)[3:]\n",
    "        Theta1 = Theta1 - (alpha * grad1)\n",
    "        Theta2 = Theta2 - (alpha * grad2)\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    nn_paramsFinal = np.append(Theta1.flatten(),Theta2.flatten())\n",
    "    return nn_paramsFinal , J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It may take very long to finish the training. \n",
    "#For 1000 iterations the Training Set Accuracy: 95.28(lambda=0.1; alpha=1)\n",
    "#You may need more iterations (e.g. 1500) to get better accuracy\n",
    "\n",
    "input_layer_size  = x_train.shape[1]\n",
    "hidden_layer_size = 30\n",
    "\n",
    "alpha=1 #learning rate\n",
    "num_iters=50\n",
    "Lambda=1\n",
    "num_labels = len(classes)\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "initial_nn_params = np.append(initial_Theta1.flatten(),initial_Theta2.flatten())\n",
    "\n",
    "\n",
    "nnTheta, nnJ_history = gradientDescentnn(x_train,y_train,initial_nn_params,alpha,num_iters,Lambda,input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "Theta1 = nnTheta[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "Theta2 = nnTheta[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Accuracy: 12.28143213988343 %\n",
      "Training Set Accuracy: 36.88592839300583 %\n"
     ]
    }
   ],
   "source": [
    "pred3 = predict(Theta1, Theta2, x_val)\n",
    "print(\"Validation Set Accuracy:\",sum(pred3[:,np.newaxis]==y_val)[0]/m*100,\"%\")\n",
    "\n",
    "pred3 = predict(Theta1, Theta2, x_train)\n",
    "print(\"Training Set Accuracy:\",sum(pred3[:,np.newaxis]==y_train)[0]/m*100,\"%\")\n",
    "\n",
    "# 12% 36%  25  1  300  1\n",
    "# 12  36   30  1  300  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Cost function using Gradient Descent')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the cost function evolution during training.\n",
    "#In order to say learning has finished, the cost function has to converge to a flat rate\n",
    "plt.plot(nnJ_history)  #\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\Theta)$\")\n",
    "plt.title(\"Cost function using Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
